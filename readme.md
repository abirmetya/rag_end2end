# ğŸ§  Retrieval-Augmented Generation (RAG) Chatbot

This repository contains an **end-to-end RAG (Retrieval-Augmented Generation) workflow** integrated into a **Streamlit web app**.  
It allows users to **upload documents, manage them, and chat with an LLM** that can reference both uploaded knowledge and prior conversations for context.

---

## ğŸš€ Features

- **Document Management**  
  - Upload documents (PDF/TXT/MD/etc.)  
  - List available documents  
  - Delete documents  

- **Chat with Context**  
  - Supports **query contextualization** (rewrites ambiguous queries using history)  
  - Stores **previous chats in SQLite** with session IDs for persistence  
  - Retrieval of relevant chunks from uploaded docs for **RAG-powered responses**  

- **Model Flexibility**  
  - Choose from multiple **LLM backends** at runtime  
  - Custom embeddings for document storage and retrieval  

- **Backend APIs (FastAPI)**  
  - `upload_document` â€“ Upload files  
  - `delete_document` â€“ Remove stored docs  
  - `list_document` â€“ Get document list  
  - `chat` â€“ Query with contextual history  

- **Frontend (Streamlit)**  
  - Simple **UI to interact with the system**  
  - Select LLM model  
  - Upload/Delete documents  
  - Chat with memory-aware RAG  

---

## ğŸ—ï¸ Architecture Overview

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Streamlit   â”‚  â‡¦ User interacts here
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
          (API Calls via HTTP)
                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚        FastAPI         â”‚
     â”‚ Endpoints:             â”‚
     â”‚  â€¢ upload_document     â”‚
     â”‚  â€¢ delete_document     â”‚
     â”‚  â€¢ list_document       â”‚
     â”‚  â€¢ chat                â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      RAG Workflow           â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚ â”‚Embedder â”‚ Retriever     â”‚ â”‚
    â”‚ â”‚ (Vector â”‚ (DB Search)  â”‚ â”‚
    â”‚ â”‚ Store)  â”‚               â”‚ â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚       â”‚                     â”‚
    â”‚       â–¼                     â”‚
    â”‚   Query Contextualizer      â”‚
    â”‚ (with SQLite Chat History)  â”‚
    â”‚       â”‚                     â”‚
    â”‚       â–¼                     â”‚
    â”‚         LLM                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## ğŸ“¦ Tech Stack

- **Frontend:** Streamlit  
- **Backend API:** FastAPI  
- **Database:** SQLite (for chat history + session IDs)  
- **Embeddings:** (OpenAI / HuggingFace / any chosen model)  
- **Vector Store:** (e.g., FAISS / Chroma / Pinecone â€” depending on config)  
- **LLMs:** (OpenAI GPT / Llama / etc. â€” user selectable)  

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/your-username/rag-chatbot.git
cd rag-chatbot

### 2ï¸âƒ£ Create Virtual Environment
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows

### 3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

### 4ï¸âƒ£ Set Environment Variables
OPENAI_API_KEY=your_key_here

â–¶ï¸ Usage
1ï¸âƒ£ Start Backend (FastAPI)
uvicorn main:app --reload --port 8000

2ï¸âƒ£ Run Frontend (Streamlit)
streamlit run app.py

3ï¸âƒ£ Interact with the App

Upload / delete documents

Choose LLM model

Ask questions and get RAG-powered answers

ğŸ“‚ API Endpoints
Method	Endpoint	Description
POST	/upload_document	Upload a new document
GET	/list_document	List all uploaded documents
DELETE	/delete_document	Delete a document
POST	/chat	Ask a question & get contextual ans
ğŸ’¾ Database (SQLite)

Table: chat_history

Columns:

id â€“ Auto increment

session_id â€“ Unique user session

user_query â€“ Raw input from user

contextualized_query â€“ Reformulated query

bot_response â€“ LLM response

timestamp â€“ Time of entry

This enables multi-session context retention.

ğŸ”® Roadmap

 Add support for multi-user authentication

 Integrate advanced chunking strategies

 Add evaluation metrics for retrieval quality

 Deploy to cloud (AWS/GCP/Azure)

ğŸ¤ Contributing

Contributions are welcome!
Please open an issue or PR for any bug fixes, feature requests, or improvements.

ğŸ“œ License

This project is licensed under the MIT License.